{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7d06396",
   "metadata": {},
   "source": [
    "# Preparation for distilation\n",
    "## calculate the baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94955b1e",
   "metadata": {},
   "source": [
    "## loade model & data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ec72106",
   "metadata": {},
   "outputs": [],
   "source": [
    "RUNPPL = True\n",
    "CLEANTEACHER = False\n",
    "LOADRUNPPL = True\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530591a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhaohongzhicai/miniconda3/envs/qwen310/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login(token=\"\")  # 替换为你的 Hugging Face 访问令牌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a05dedc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.10\n",
      "Torch: 2.9.1+cu128\n",
      "Transformers: 4.57.1\n",
      "Hub: 0.36.0\n",
      "Tokenizers: 0.22.1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch, transformers, huggingface_hub, tokenizers\n",
    "print(\"Python:\",   torch.__spec__.origin.split(\"python\")[1].split(\"/\")[0])\n",
    "print(\"Torch:\",    torch.__version__)\n",
    "print(\"Transformers:\", transformers.__version__)\n",
    "print(\"Hub:\", huggingface_hub.__version__)\n",
    "print(\"Tokenizers:\", tokenizers.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16630793",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhaohongzhicai/miniconda3/envs/qwen310/lib/python3.10/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import transformers\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"Nan-Do/code-search-net-python\", split=\"train\").select(range(20000)) # 选择部分样本用于蒸馏 20000\n",
    "dataset2 = load_dataset(\"Nan-Do/code-search-net-python\", split=\"train\").select(range(20001,21001)) # 1000 样本用于测试 PPL\n",
    "\n",
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "\n",
    "# student_device = torch.device(\"cuda:0\")\n",
    "# teacher_device = torch.device(\"cuda:0\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "615b4327",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig  # 引入 BitsAndBytesConfig\n",
    "import torch\n",
    "QWEN2point5_1point5B_MODEL_NAME = \"Qwen/Qwen2.5-1.5B\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(QWEN2point5_1point5B_MODEL_NAME, trust_remote_code=True)\n",
    "\n",
    "# model_1_5b = AutoModelForCausalLM.from_pretrained(\n",
    "#     QWEN2point5_1point5B_MODEL_NAME,\n",
    "#     torch_dtype=torch.bfloat16,          # 新版 transformers 用 dtype，就按 warning 说的\n",
    "#     trust_remote_code=True,\n",
    "#     device_map=device\n",
    "# )\n",
    "model_1_5b = AutoModelForCausalLM.from_pretrained(\n",
    "    QWEN2point5_1point5B_MODEL_NAME,\n",
    "    quantization_config=bnb_config,  # <--- 传入量化配置\n",
    "    torch_dtype=torch.bfloat16,      \n",
    "    trust_remote_code=True,\n",
    "    device_map=device                # 或者是 \"auto\"\n",
    ")           \n",
    "teacher = model_1_5b\n",
    "print(teacher.device)              # 确认一下\n",
    "tokenizer = AutoTokenizer.from_pretrained(QWEN2point5_1point5B_MODEL_NAME, trust_remote_code=True)\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669337c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "def compute_ppl_batch(model, tokenizer, dataset, num_samples=100, target_gpu=torch.device(\"cuda:0\")):\n",
    "    total_ppl = 0.0\n",
    "    device = target_gpu\n",
    "    for i in tqdm(range(num_samples)):\n",
    "        text = dataset[i][\"code\"]\n",
    "        enc = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "        with torch.no_grad():\n",
    "            out = model(**enc, labels=enc[\"input_ids\"])\n",
    "        loss = out.loss\n",
    "        total_ppl += torch.exp(loss).item()\n",
    "    return total_ppl / num_samples\n",
    "if RUNPPL:\n",
    "    print(compute_ppl_batch(model_1_5b, tokenizer, dataset2, target_gpu=device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d3c441",
   "metadata": {},
   "source": [
    "3.9,6.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7758b5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if CLEANTEACHER:\n",
    "    del model_1_5b\n",
    "    del tokenizer\n",
    "    import gc\n",
    "    gc.collect() \n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "affee3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "QWEN2point5_0point5B_MODEL_NAME = \"Qwen/Qwen2.5-0.5B\"\n",
    "\n",
    "\n",
    "model_0_5b = AutoModelForCausalLM.from_pretrained(\n",
    "    QWEN2point5_0point5B_MODEL_NAME,\n",
    "    # device_map=\"cuda:0\",# for PPL\n",
    "    device_map=device, \n",
    "    dtype=torch.float32,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "875733aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:03<00:00, 30.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.877563061714172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if RUNPPL:\n",
    "    print(compute_ppl_batch(model_0_5b, tokenizer, dataset2,target_gpu=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c250dd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = \"Write a short Python function to compute Fibonacci numbers.\"\n",
    "# inputs = tokenizer_0_6b(prompt, return_tensors=\"pt\").to(model_0_6b.device)\n",
    "# with torch.no_grad():\n",
    "#     outputs = model_0_6b.generate(**inputs, max_new_tokens=200)\n",
    "# print(tokenizer_0_6b.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7cdeddaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Model Layers ---\n",
      "模型名称: Qwen/Qwen2.5-1.5B\n",
      "Transformer 层数: 28\n",
      "Qwen2ForCausalLM(\n",
      "  (model): Qwen2Model(\n",
      "    (embed_tokens): Embedding(151936, 1536)\n",
      "    (layers): ModuleList(\n",
      "      (0-27): 28 x Qwen2DecoderLayer(\n",
      "        (self_attn): Qwen2Attention(\n",
      "          (q_proj): Linear8bitLt(in_features=1536, out_features=1536, bias=True)\n",
      "          (k_proj): Linear8bitLt(in_features=1536, out_features=256, bias=True)\n",
      "          (v_proj): Linear8bitLt(in_features=1536, out_features=256, bias=True)\n",
      "          (o_proj): Linear8bitLt(in_features=1536, out_features=1536, bias=False)\n",
      "        )\n",
      "        (mlp): Qwen2MLP(\n",
      "          (gate_proj): Linear8bitLt(in_features=1536, out_features=8960, bias=False)\n",
      "          (up_proj): Linear8bitLt(in_features=1536, out_features=8960, bias=False)\n",
      "          (down_proj): Linear8bitLt(in_features=8960, out_features=1536, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "        (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "    (rotary_emb): Qwen2RotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=1536, out_features=151936, bias=False)\n",
      ")\n",
      "--------------------\n",
      "--- Model Layers ---\n",
      "模型名称: Qwen/Qwen2.5-0.5B\n",
      "Transformer 层数: 24\n",
      "Qwen2ForCausalLM(\n",
      "  (model): Qwen2Model(\n",
      "    (embed_tokens): Embedding(151936, 896)\n",
      "    (layers): ModuleList(\n",
      "      (0-23): 24 x Qwen2DecoderLayer(\n",
      "        (self_attn): Qwen2Attention(\n",
      "          (q_proj): Linear(in_features=896, out_features=896, bias=True)\n",
      "          (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
      "          (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
      "          (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
      "        )\n",
      "        (mlp): Qwen2MLP(\n",
      "          (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
      "          (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
      "          (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "    (rotary_emb): Qwen2RotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoConfig\n",
    "\n",
    "def print_model_layers(model_name):\n",
    "    try:\n",
    "        config = AutoConfig.from_pretrained(\n",
    "            model_name,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading config for {model_name}: {e}\")\n",
    "        return\n",
    "\n",
    "    if hasattr(config, \"num_hidden_layers\"):\n",
    "        num_layers = config.num_hidden_layers\n",
    "        print(\"--- Model Layers ---\")\n",
    "        print(f\"模型名称: {model_name}\")\n",
    "        print(f\"Transformer 层数: {num_layers}\")\n",
    "    else:\n",
    "        print(f\"在 {model_name} 的配置中未找到 'num_hidden_layers' 属性。\")\n",
    "\n",
    "print_model_layers(QWEN2point5_1point5B_MODEL_NAME)\n",
    "print(model_1_5b)\n",
    "print(\"-\" * 20)\n",
    "print_model_layers(QWEN2point5_0point5B_MODEL_NAME)\n",
    "print(model_0_5b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08a818e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "from transformers import DataCollatorForLanguageModeling, get_scheduler\n",
    "from dataclasses import dataclass\n",
    "from tqdm.auto import tqdm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DistillConfig:\n",
    "    temperature: float = 2.0\n",
    "    alpha_kd: float = 0.5       # logits KD 权重 λ_kd 0.5\n",
    "    alpha_feat: float = 0.1     # hidden feature 对齐权重 λ_feat\n",
    "    alpha_param: float = 0.0    # 参数重构正则权重 λ_param（先关掉）\n",
    "    use_feat_loss: bool = True  # 是否使用特征蒸馏（只对齐最后一层）\n",
    "    use_param_loss: bool = False  # 是否使用参数正则（权重对齐）\n",
    "\n",
    "distill_cfg = DistillConfig(\n",
    "    temperature=2.0,\n",
    "    alpha_kd=0.5,\n",
    "    alpha_feat=1,\n",
    "    alpha_param=0.0,          # 默认不打开参数正则，有需要再开\n",
    "    use_feat_loss=True,\n",
    "    use_param_loss=False,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "teacher = model_1_5b               # 假设已经是 int8 + device_map={0,1}\n",
    "student = model_0_5b             # MPO 学生模型（Qwen 0.6B MPO 版）\n",
    "\n",
    "# teacher = teacher.to(teacher_device)\n",
    "# student = student.to(student_device)\n",
    "student_dtype = student.dtype\n",
    "\n",
    "\n",
    "# teacher 不更新\n",
    "teacher.eval()\n",
    "for p in teacher.parameters():\n",
    "    p.requires_grad_(False)\n",
    "\n",
    "\n",
    "max_len = 256\n",
    "batch_size = 1\n",
    "gradient_accumulation_steps = 32\n",
    "num_epochs = 1\n",
    "lr = 1e-5\n",
    "\n",
    "raw_train = dataset\n",
    "def tokenize_fn(batch):\n",
    "    out = tokenizer(\n",
    "        batch[\"code\"],\n",
    "        truncation=True,\n",
    "        max_length=max_len,\n",
    "        padding=\"max_length\",\n",
    "        return_attention_mask=True,\n",
    "    )\n",
    "    # 非 padding token 才参与 CE\n",
    "    out[\"labels\"] = [\n",
    "        [tid if m == 1 else -100 for tid, m in zip(ids, mask)]\n",
    "        for ids, mask in zip(out[\"input_ids\"], out[\"attention_mask\"])\n",
    "    ]\n",
    "    return out\n",
    "\n",
    "train_ds = raw_train.map(\n",
    "    tokenize_fn,\n",
    "    batched=True,\n",
    "    remove_columns=raw_train.column_names,\n",
    ")\n",
    "train_ds.set_format(\n",
    "    type=\"torch\",\n",
    "    columns=[\"input_ids\", \"attention_mask\", \"labels\"],\n",
    ")\n",
    "\n",
    "collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=collator,\n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "# =====================================================\n",
    "# projection（teacher hidden -> student hidden）\n",
    "# =====================================================\n",
    "\n",
    "teacher_hidden = teacher.config.hidden_size\n",
    "student_hidden = student.config.hidden_size\n",
    "\n",
    "if teacher_hidden != student_hidden:\n",
    "    projector = nn.Linear(\n",
    "        teacher_hidden,\n",
    "        student_hidden,\n",
    "        bias=False,\n",
    "        device=device,\n",
    "        dtype=student_dtype,\n",
    "    )\n",
    "else:\n",
    "    projector = nn.Identity().to(device)\n",
    "\n",
    "# =====================================================\n",
    "# 参数正则：只针对一小部分层（示例：lm_head）\n",
    "# 先构造一个 param_pairs 列表，用于 L_param\n",
    "# =====================================================\n",
    "\n",
    "param_pairs = []\n",
    "if distill_cfg.use_param_loss and distill_cfg.alpha_param > 0.0:\n",
    "    # 示例：对齐 lm_head 权重（按你模型的真实名字来改）\n",
    "    with torch.no_grad():\n",
    "        t_lm_head_w = teacher.lm_head.weight.to(device, dtype=student_dtype)\n",
    "    param_pairs.append((student.lm_head.weight, t_lm_head_w))\n",
    "\n",
    "# =====================================================\n",
    "# 定义各个 loss 模块\n",
    "# =====================================================\n",
    "\n",
    "mse = nn.MSELoss()\n",
    "\n",
    "def compute_ce_loss(student_output):\n",
    "    \"\"\"\n",
    "    L_CE：student 对真实 token 的交叉熵。\n",
    "    HF 的 causal LM 模型在 forward 时已经返回了 loss。\n",
    "    \"\"\"\n",
    "    return student_output.loss\n",
    "\n",
    "def compute_kd_loss(student_logits, teacher_logits, T, attention_mask):\n",
    "    \"\"\"\n",
    "    修正版：只计算非 Padding 部分的 KD Loss\n",
    "    \"\"\"\n",
    "    # 1. 计算软标签概率\n",
    "    s = (student_logits / T).log_softmax(dim=-1)\n",
    "    t = (teacher_logits / T).softmax(dim=-1)\n",
    "    \n",
    "    # 2. 计算逐个 Token 的 KL 散度 [Batch, Seq_Len]\n",
    "    # KL = sum(p * (log_p - log_q)) = sum(p * log_p - p * log_q)\n",
    "    # 这里用简化的 sum(-t * s) 形式（忽略 t*log_t 常数项对梯度的影响）\n",
    "    kl_pointwise = torch.sum(-t * s, dim=-1) \n",
    "    \n",
    "    # 3. 应用 Mask [Batch, Seq_Len]\n",
    "    # attention_mask 形状通常是 [Batch, Seq_Len]，1为有效，0为Pad\n",
    "    mask = attention_mask.to(kl_pointwise.dtype)\n",
    "    \n",
    "    # 4. 只取有效 Token 的平均值\n",
    "    loss = (kl_pointwise * mask).sum() / mask.sum().clamp_min(1.0)\n",
    "    \n",
    "    return loss * (T * T)\n",
    "\n",
    "\n",
    "def compute_feat_loss(s_last_hidden, t_last_hidden_projected, attention_mask=None):\n",
    "    # 对特征进行 L2 归一化，消除模长影响，只关注方向一致性\n",
    "    s_norm = F.normalize(s_last_hidden, p=2, dim=-1)\n",
    "    t_norm = F.normalize(t_last_hidden_projected, p=2, dim=-1)\n",
    "    \n",
    "    if attention_mask is None:\n",
    "        return mse(s_norm, t_norm)\n",
    "\n",
    "    mask = attention_mask.unsqueeze(-1).to(s_last_hidden.dtype)\n",
    "    s_masked = s_norm * mask\n",
    "    t_masked = t_norm * mask\n",
    "    \n",
    "    # 归一化后的 MSE，数值通常很小 (<1.0)\n",
    "    denom = mask.sum().clamp_min(1.0)\n",
    "    # 注意：归一化后 MSE = 2 * (1 - cosine_similarity)\n",
    "    return ((s_masked - t_masked) ** 2).sum() / denom\n",
    "\n",
    "def compute_param_loss(param_pairs):\n",
    "    \"\"\"\n",
    "    L_param：参数正则（权重对齐）。\n",
    "    param_pairs: List[(W_student, W_teacher_fixed)]\n",
    "    通常只选很少的一两层做（例如 lm_head 或某几层 MPO）。\n",
    "    \"\"\"\n",
    "    if not param_pairs:\n",
    "        return torch.tensor(0.0, device=device)\n",
    "\n",
    "    loss = torch.tensor(0.0, device=device)\n",
    "    for w_s, w_t in param_pairs:\n",
    "        loss = loss + ((w_s - w_t) ** 2).mean()\n",
    "    return loss\n",
    "\n",
    "# =====================================================\n",
    "# 优化器 & Scheduler\n",
    "# =====================================================\n",
    "num_update_steps_per_epoch = len(train_loader) // gradient_accumulation_steps\n",
    "num_training_steps = num_epochs * num_update_steps_per_epoch\n",
    "optimizer = torch.optim.AdamW(\n",
    "    list(student.parameters()) + list(projector.parameters()),\n",
    "    lr=lr, # 建议保持 1e-5 或稍微降低到 5e-6\n",
    ")\n",
    "\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=100,            # <--- 在这里设置 Warmup\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "# =====================================================\n",
    "# 训练循环：双 GPU 蒸馏\n",
    "#   - teacher 用 device_map 多卡，只出 logits (+ 可选 hidden)\n",
    "#   - student 固定在 cuda:1\n",
    "# =====================================================\n",
    "\n",
    "student.train()\n",
    "projector.train()\n",
    "\n",
    "progress = tqdm(range(num_training_steps))\n",
    "# 确保在循环开始前清空梯度\n",
    "optimizer.zero_grad()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for step, batch in enumerate(train_loader):\n",
    "        # ... (数据搬运部分保持不变) ...\n",
    "        batch_cpu = {k: v for k, v in batch.items()}\n",
    "        batch_student = {k: v.to(device, non_blocking=True) for k, v in batch_cpu.items()}\n",
    "        attention_mask = batch_student[\"attention_mask\"]\n",
    "\n",
    "        # ... (Teacher Forward 保持不变) ...\n",
    "        with torch.no_grad():\n",
    "            batch_teacher = {\n",
    "                k: v.to(device, non_blocking=True)\n",
    "                for k, v in batch_cpu.items()\n",
    "                if k in (\"input_ids\", \"attention_mask\")\n",
    "            }\n",
    "            t_out = teacher(\n",
    "                **batch_teacher,\n",
    "                output_hidden_states=distill_cfg.use_feat_loss,\n",
    "                use_cache=False,\n",
    "            )\n",
    "            t_logits = t_out.logits\n",
    "            t_last_hidden = t_out.hidden_states[-1] if distill_cfg.use_feat_loss else None\n",
    "\n",
    "        # ... (Student Forward 保持不变) ...\n",
    "        s_out = student(\n",
    "            **batch_student,\n",
    "            output_hidden_states=distill_cfg.use_feat_loss,\n",
    "            use_cache=False,\n",
    "        )\n",
    "        s_logits = s_out.logits\n",
    "        s_last_hidden = s_out.hidden_states[-1] if distill_cfg.use_feat_loss else None\n",
    "\n",
    "        # ... (Loss 计算保持不变) ...\n",
    "        loss_ce = compute_ce_loss(s_out)\n",
    "        \n",
    "        # 修正版 KD (带 mask)\n",
    "        loss_kd = compute_kd_loss(\n",
    "            s_logits, \n",
    "            t_logits, \n",
    "            T=distill_cfg.temperature, \n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "\n",
    "        # 归一化版 Feat\n",
    "        if distill_cfg.use_feat_loss and t_last_hidden is not None:\n",
    "            # === 核心修改：强制转换 Teacher 的输出类型以匹配 Projector ===\n",
    "            t_last_hidden_cast = t_last_hidden.to(dtype=projector.weight.dtype) \n",
    "            \n",
    "            t_last_proj = projector(t_last_hidden_cast)\n",
    "            \n",
    "            loss_feat = compute_feat_loss(\n",
    "                s_last_hidden,\n",
    "                t_last_proj,\n",
    "                attention_mask=attention_mask,\n",
    "            )\n",
    "        else:\n",
    "            loss_feat = torch.tensor(0.0, device=device)\n",
    "\n",
    "        if distill_cfg.use_param_loss:\n",
    "            loss_param = compute_param_loss(param_pairs)\n",
    "        else:\n",
    "            loss_param = torch.tensor(0.0, device=device)\n",
    "\n",
    "        # 总 Loss\n",
    "        loss = (\n",
    "            loss_ce\n",
    "            + distill_cfg.alpha_kd * loss_kd\n",
    "            + distill_cfg.alpha_feat * loss_feat\n",
    "            + distill_cfg.alpha_param * loss_param\n",
    "        )\n",
    "\n",
    "        if torch.isnan(loss):\n",
    "            print(f\"Skipping step {step} due to NaN loss\")\n",
    "            continue\n",
    "\n",
    "        # ---------- 6. 反向 & 更新 (逻辑修正) ----------\n",
    "        \n",
    "        # 1. Loss 归一化 (分摊到每一步)\n",
    "        loss = loss / gradient_accumulation_steps\n",
    "        \n",
    "        # 2. 反向传播 (每一步都做，梯度会累积在 .grad 中)\n",
    "        loss.backward()\n",
    "\n",
    "        # 3. 参数更新 (只在累积够了步数后做)\n",
    "        if (step + 1) % gradient_accumulation_steps == 0:\n",
    "            # 梯度裁剪\n",
    "            torch.nn.utils.clip_grad_norm_(student.parameters(), 1.0)\n",
    "            \n",
    "            # 更新参数\n",
    "            optimizer.step()\n",
    "            \n",
    "            # 更新学习率\n",
    "            lr_scheduler.step()\n",
    "            \n",
    "            # 清空梯度 (为下一轮累积做准备)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # 更新进度条\n",
    "            progress.update(1)\n",
    "            progress.set_postfix({\n",
    "                \"loss\": loss.item() * gradient_accumulation_steps, # 还原数值方便观察\n",
    "                \"ce\": loss_ce.item(),\n",
    "                \"kd\": loss_kd.item(),\n",
    "                \"feat\": loss_feat.item(),\n",
    "                \"lr\": optimizer.param_groups[0]['lr']\n",
    "            })\n",
    "\n",
    "# =====================================================\n",
    "# 保存蒸馏后的 MPO 学生\n",
    "# =====================================================\n",
    "\n",
    "print(compute_ppl_batch(model_0_5b, tokenizer, dataset2,target_gpu=device))\n",
    "\n",
    "# save_dir = \"distilled_qwen2_0_5b_kd_modular\"\n",
    "# student.save_pretrained(save_dir)\n",
    "# tokenizer.save_pretrained(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "76d4532b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "625\n"
     ]
    }
   ],
   "source": [
    "print(num_update_steps_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "592577e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved files: ['distilled_qwen2_0_5b_mmp_kd_modular/merges.txt', 'distilled_qwen2_0_5b_mmp_kd_modular/special_tokens_map.json', 'distilled_qwen2_0_5b_mmp_kd_modular/generation_config.json', 'distilled_qwen2_0_5b_mmp_kd_modular/vocab.json', 'distilled_qwen2_0_5b_mmp_kd_modular/added_tokens.json', 'distilled_qwen2_0_5b_mmp_kd_modular/tokenizer_config.json', 'distilled_qwen2_0_5b_mmp_kd_modular/tokenizer.json', 'distilled_qwen2_0_5b_mmp_kd_modular/model.safetensors', 'distilled_qwen2_0_5b_mmp_kd_modular/chat_template.jinja', 'distilled_qwen2_0_5b_mmp_kd_modular/config.json']\n",
      "Found safetensors files: ['distilled_qwen2_0_5b_mmp_kd_modular/model.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:03<00:00, 29.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.208931834697723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if LOADRUNPPL:\n",
    "        \n",
    "    import os, glob\n",
    "    from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "    import torch\n",
    "    from datasets import load_dataset\n",
    "\n",
    "    save_dir = \"distilled_qwen2_0_5b_mmp_kd_modular\"  # 替换为你的保存目录\n",
    "\n",
    "    # 1) Inspect the save directory for safetensors/pytorch files\n",
    "    files = glob.glob(os.path.join(save_dir, \"*\"))\n",
    "    print(\"Saved files:\", files)\n",
    "    safetensors_files = [f for f in files if f.endswith(\".safetensors\")]\n",
    "    if safetensors_files:\n",
    "        print(\"Found safetensors files:\", safetensors_files)\n",
    "    else:\n",
    "        print(\"No .safetensors files found. Looking for pytorch weights...\")\n",
    "        pt_files = [f for f in files if \"pytorch_model\" in f or f.endswith(\".bin\")]\n",
    "        print(\"Pytorch files:\", pt_files)\n",
    "\n",
    "    # 2) Load the tokenizer\n",
    "    tokenizer_distilled = AutoTokenizer.from_pretrained(save_dir, trust_remote_code=True)\n",
    "    if tokenizer_distilled.pad_token is None:\n",
    "        tokenizer_distilled.pad_token = tokenizer_distilled.eos_token\n",
    "\n",
    "    # 3) Load the model (AutoModelForCausalLM handles safetensors automatically if present)\n",
    "    # Use device_map to put weights on the same device as `device` variable defined in the notebook.\n",
    "    # If you want to load to CPU first, set device_map=\"cpu\"\n",
    "    try:\n",
    "        model_distilled = AutoModelForCausalLM.from_pretrained(\n",
    "            save_dir,\n",
    "            trust_remote_code=True,\n",
    "            device_map=device,   # uses device variable from the notebook (e.g., \"cuda:0\")\n",
    "            low_cpu_mem_usage=True\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(\"Loading with device_map=device failed:\", e)\n",
    "        print(\"Retrying with device_map='auto'...\")\n",
    "        model_distilled = AutoModelForCausalLM.from_pretrained(\n",
    "            save_dir,\n",
    "            trust_remote_code=True,\n",
    "            device_map=\"auto\",\n",
    "            low_cpu_mem_usage=True\n",
    "        )\n",
    "\n",
    "    print(compute_ppl_batch(model_distilled, tokenizer_distilled, dataset2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a0b0f007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss = alpha_kd * loss_kd + alpha_feat * feat_loss + (1 - alpha_kd - alpha_feat) * loss_lm\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qwen310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
