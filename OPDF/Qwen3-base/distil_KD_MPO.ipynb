{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7d06396",
   "metadata": {},
   "source": [
    "# Preparation for distilation\n",
    "## calculate the baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94955b1e",
   "metadata": {},
   "source": [
    "## loade model & data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ec72106",
   "metadata": {},
   "outputs": [],
   "source": [
    "RUNPPL = True\n",
    "CLEANTEACHER = False\n",
    "LOADRUNPPL = False\n",
    "ONLYTRAINMPO = True\n",
    "SAVEMODEL = True\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530591a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhaohongzhicai/miniconda3/envs/qwen310/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login(token=\"\")  # 替换为你的 Hugging Face 访问令牌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a05dedc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.10\n",
      "Torch: 2.9.1+cu128\n",
      "Transformers: 4.57.1\n",
      "Hub: 0.36.0\n",
      "Tokenizers: 0.22.1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch, transformers, huggingface_hub, tokenizers\n",
    "print(\"Python:\",   torch.__spec__.origin.split(\"python\")[1].split(\"/\")[0])\n",
    "print(\"Torch:\",    torch.__version__)\n",
    "print(\"Transformers:\", transformers.__version__)\n",
    "print(\"Hub:\", huggingface_hub.__version__)\n",
    "print(\"Tokenizers:\", tokenizers.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16630793",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhaohongzhicai/miniconda3/envs/qwen310/lib/python3.10/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import transformers\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"Nan-Do/code-search-net-python\", split=\"train\").select(range(20000)) # 选择部分样本用于蒸馏 20000\n",
    "dataset2 = load_dataset(\"Nan-Do/code-search-net-python\", split=\"train\").select(range(20001,21001)) # 1000 样本用于测试 PPL\n",
    "\n",
    "device = torch.device(\"cuda:1\")\n",
    "\n",
    "# student_device = torch.device(\"cuda:0\")\n",
    "# teacher_device = torch.device(\"cuda:0\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "615b4327",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig  # 引入 BitsAndBytesConfig\n",
    "import torch\n",
    "QWEN2point5_1point5B_MODEL_NAME = \"Qwen/Qwen2.5-1.5B\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(QWEN2point5_1point5B_MODEL_NAME, trust_remote_code=True)\n",
    "\n",
    "# model_1_5b = AutoModelForCausalLM.from_pretrained(\n",
    "#     QWEN2point5_1point5B_MODEL_NAME,\n",
    "#     torch_dtype=torch.bfloat16,          # 新版 transformers 用 dtype，就按 warning 说的\n",
    "#     trust_remote_code=True,\n",
    "#     device_map=device\n",
    "# )\n",
    "model_1_5b = AutoModelForCausalLM.from_pretrained(\n",
    "    QWEN2point5_1point5B_MODEL_NAME,\n",
    "    quantization_config=bnb_config,  # <--- 传入量化配置\n",
    "    torch_dtype=torch.bfloat16,      \n",
    "    trust_remote_code=True,\n",
    "    device_map=device                # 或者是 \"auto\"\n",
    ")           \n",
    "teacher = model_1_5b\n",
    "print(teacher.device)              # 确认一下\n",
    "tokenizer = AutoTokenizer.from_pretrained(QWEN2point5_1point5B_MODEL_NAME, trust_remote_code=True)\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "669337c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]/home/zhaohongzhicai/miniconda3/envs/qwen310/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:181: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "100%|██████████| 100/100 [00:13<00:00,  7.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.09707392692566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "def compute_ppl_batch(model, tokenizer, dataset, num_samples=100, target_gpu=torch.device(\"cuda:0\")):\n",
    "    total_ppl = 0.0\n",
    "    device = target_gpu\n",
    "    for i in tqdm(range(num_samples)):\n",
    "        text = dataset[i][\"code\"]\n",
    "        enc = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "        with torch.no_grad():\n",
    "            out = model(**enc, labels=enc[\"input_ids\"])\n",
    "        loss = out.loss\n",
    "        total_ppl += torch.exp(loss).item()\n",
    "    return total_ppl / num_samples\n",
    "if RUNPPL:\n",
    "    print(compute_ppl_batch(model_1_5b, tokenizer, dataset2, target_gpu=device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d3c441",
   "metadata": {},
   "source": [
    "3.9,6.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7758b5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if CLEANTEACHER:\n",
    "    del model_1_5b\n",
    "    del tokenizer\n",
    "    import gc\n",
    "    gc.collect() \n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "affee3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "QWEN2point5_0point5B_MODEL_NAME = \"Qwen/Qwen2.5-0.5B\"\n",
    "\n",
    "\n",
    "model_0_5b = AutoModelForCausalLM.from_pretrained(\n",
    "    QWEN2point5_0point5B_MODEL_NAME,\n",
    "    # device_map=\"cuda:0\",# for PPL\n",
    "    device_map=device, \n",
    "    dtype=torch.float32,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "875733aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:03<00:00, 30.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.877563061714172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if RUNPPL:\n",
    "    print(compute_ppl_batch(model_0_5b, tokenizer, dataset2,target_gpu=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c250dd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = \"Write a short Python function to compute Fibonacci numbers.\"\n",
    "# inputs = tokenizer(prompt, return_tensors=\"pt\").to(model_0_6b.device)\n",
    "# with torch.no_grad():\n",
    "#     outputs = student.generate(**inputs, max_new_tokens=200)\n",
    "# print(tokenizer_0_6b.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7cdeddaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Model Layers ---\n",
      "模型名称: Qwen/Qwen2.5-1.5B\n",
      "Transformer 层数: 28\n",
      "Qwen2ForCausalLM(\n",
      "  (model): Qwen2Model(\n",
      "    (embed_tokens): Embedding(151936, 1536)\n",
      "    (layers): ModuleList(\n",
      "      (0-27): 28 x Qwen2DecoderLayer(\n",
      "        (self_attn): Qwen2Attention(\n",
      "          (q_proj): Linear8bitLt(in_features=1536, out_features=1536, bias=True)\n",
      "          (k_proj): Linear8bitLt(in_features=1536, out_features=256, bias=True)\n",
      "          (v_proj): Linear8bitLt(in_features=1536, out_features=256, bias=True)\n",
      "          (o_proj): Linear8bitLt(in_features=1536, out_features=1536, bias=False)\n",
      "        )\n",
      "        (mlp): Qwen2MLP(\n",
      "          (gate_proj): Linear8bitLt(in_features=1536, out_features=8960, bias=False)\n",
      "          (up_proj): Linear8bitLt(in_features=1536, out_features=8960, bias=False)\n",
      "          (down_proj): Linear8bitLt(in_features=8960, out_features=1536, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "        (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "    (rotary_emb): Qwen2RotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=1536, out_features=151936, bias=False)\n",
      ")\n",
      "--------------------\n",
      "--- Model Layers ---\n",
      "模型名称: Qwen/Qwen2.5-0.5B\n",
      "Transformer 层数: 24\n",
      "Qwen2ForCausalLM(\n",
      "  (model): Qwen2Model(\n",
      "    (embed_tokens): Embedding(151936, 896)\n",
      "    (layers): ModuleList(\n",
      "      (0-23): 24 x Qwen2DecoderLayer(\n",
      "        (self_attn): Qwen2Attention(\n",
      "          (q_proj): Linear(in_features=896, out_features=896, bias=True)\n",
      "          (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
      "          (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
      "          (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
      "        )\n",
      "        (mlp): Qwen2MLP(\n",
      "          (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
      "          (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
      "          (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "    (rotary_emb): Qwen2RotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoConfig\n",
    "\n",
    "def print_model_layers(model_name):\n",
    "    try:\n",
    "        config = AutoConfig.from_pretrained(\n",
    "            model_name,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading config for {model_name}: {e}\")\n",
    "        return\n",
    "\n",
    "    if hasattr(config, \"num_hidden_layers\"):\n",
    "        num_layers = config.num_hidden_layers\n",
    "        print(\"--- Model Layers ---\")\n",
    "        print(f\"模型名称: {model_name}\")\n",
    "        print(f\"Transformer 层数: {num_layers}\")\n",
    "    else:\n",
    "        print(f\"在 {model_name} 的配置中未找到 'num_hidden_layers' 属性。\")\n",
    "\n",
    "print_model_layers(QWEN2point5_1point5B_MODEL_NAME)\n",
    "print(model_1_5b)\n",
    "print(\"-\" * 20)\n",
    "print_model_layers(QWEN2point5_0point5B_MODEL_NAME)\n",
    "print(model_0_5b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "24259e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import gc  # 引入垃圾回收模块\n",
    "\n",
    "class OverParameterizedLinear(nn.Module):\n",
    "    def __init__(self, original_linear, expansion_factor=1.5):\n",
    "        super().__init__()\n",
    "        self.in_features = original_linear.in_features\n",
    "        self.out_features = original_linear.out_features\n",
    "        self.bias = original_linear.bias\n",
    "        \n",
    "        # 计算中间维度\n",
    "        self.mid_dim = int(min(self.in_features, self.out_features) * expansion_factor)\n",
    "        \n",
    "        device = original_linear.weight.device\n",
    "        dtype = original_linear.weight.dtype\n",
    "\n",
    "        # ============================================================\n",
    "        # 优化版 SVD 初始化 (Asymmetric SVD + Error Check)\n",
    "        # ============================================================\n",
    "        \n",
    "        # 1. 强制使用 float32 进行高精度分解\n",
    "        print(f\"  > Initializing MPO layer: {self.in_features} -> {self.mid_dim} -> {self.out_features}\")\n",
    "        w_orig_cpu = original_linear.weight.data.float().cpu()\n",
    "        \n",
    "        # 2. SVD 分解\n",
    "        U, S, Vh = torch.linalg.svd(w_orig_cpu, full_matrices=False)\n",
    "        \n",
    "        # 3. 截断与分配 (非对称分配，减少 sqrt 带来的精度损失)\n",
    "        r = min(self.in_features, self.out_features)\n",
    "        \n",
    "        # 方案：A = U, B = S @ Vh\n",
    "        # 这样 A 是正交矩阵，数值非常稳定；B 承载了幅值\n",
    "        A_init = U[:, :r] \n",
    "        B_init = torch.diag(S[:r]) @ Vh[:r, :]\n",
    "        \n",
    "        # 4. 初始化参数\n",
    "        # A: 随机初始化 (提供梯度路径)\n",
    "        # B: 全 0 初始化 (关键！确保 expansion 部分对结果无影响)\n",
    "        self.matrix_a = nn.Parameter(torch.randn(self.out_features, self.mid_dim, device=device) * 0.001)\n",
    "        self.matrix_b = nn.Parameter(torch.zeros(self.mid_dim, self.in_features, device=device))\n",
    "        \n",
    "        # 5. 填入 SVD 结果\n",
    "        with torch.no_grad():\n",
    "            # 将 numpy/cpu 结果转回目标设备和精度\n",
    "            self.matrix_a.data[:, :r] = A_init.to(device=device, dtype=dtype)\n",
    "            self.matrix_b.data[:r, :] = B_init.to(device=device, dtype=dtype)\n",
    "            \n",
    "            # === 自检环节：验证初始化误差 ===\n",
    "            # 计算重构矩阵\n",
    "            W_recon = self.matrix_a.data @ self.matrix_b.data\n",
    "            W_orig = original_linear.weight.data\n",
    "            \n",
    "            # 计算相对误差 (Frobenius Norm)\n",
    "            diff = (W_recon - W_orig).float().norm()\n",
    "            total = W_orig.float().norm()\n",
    "            rel_error = diff / total\n",
    "            \n",
    "            print(f\"  > Reconstruction Error: {rel_error.item():.6f}\")\n",
    "            if rel_error > 1e-2: # 如果误差大于 1%，说明初始化失败\n",
    "                print(\"  !!! WARNING: High reconstruction error. Check dtype or SVD logic.\")\n",
    "\n",
    "        # 6. 清理\n",
    "        del w_orig_cpu, U, S, Vh, A_init, B_init, W_recon\n",
    "        gc.collect()\n",
    "        if device.type == 'cuda':\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        # 同步精度\n",
    "        self.to(device, dtype=dtype)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_mid = F.linear(x, self.matrix_b)\n",
    "        out = F.linear(x_mid, self.matrix_a, self.bias)\n",
    "        return out\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def merge_to_linear(self):\n",
    "        merged_weight = self.matrix_a @ self.matrix_b\n",
    "        new_linear = nn.Linear(self.in_features, self.out_features, bias=self.bias is not None)\n",
    "        new_linear.weight.data = merged_weight\n",
    "        if self.bias is not None:\n",
    "            new_linear.bias.data = self.bias\n",
    "        return new_linear\n",
    "    \n",
    "# 2. 专门针对 Qwen2.5 的替换函数\n",
    "def apply_mpo_to_qwen(model, num_layers_to_replace=2, expansion_factor=1.5):\n",
    "    \"\"\"\n",
    "    只替换 Qwen 模型最后几层的 MLP。\n",
    "    \"\"\"\n",
    "    # Qwen 的层列表通常在 model.model.layers\n",
    "    layers = model.model.layers\n",
    "    total_layers = len(layers)\n",
    "    start_layer = total_layers - num_layers_to_replace\n",
    "    \n",
    "    print(f\">>> 正在将 Layer {start_layer} 到 Layer {total_layers-1} 的 MLP 转换为超参数化层...\")\n",
    "    print(f\">>> 扩展倍率: {expansion_factor}x\")\n",
    "\n",
    "    for i in range(start_layer, total_layers):\n",
    "        layer = layers[i]\n",
    "        # Qwen2.5 的 MLP 包含三个线性层: gate_proj, up_proj, down_proj\n",
    "        # 我们全部替换，或者为了省显存只替换 down_proj (它是输出层，最关键)\n",
    "        \n",
    "        # 这里演示全部替换 (最强效果)\n",
    "        # 注意：gate_proj 和 up_proj 维度通常很大，如果显存不够，可以把 expansion_factor 调小\n",
    "        layer.mlp.gate_proj = OverParameterizedLinear(layer.mlp.gate_proj, expansion_factor)\n",
    "        layer.mlp.up_proj = OverParameterizedLinear(layer.mlp.up_proj, expansion_factor)\n",
    "        layer.mlp.down_proj = OverParameterizedLinear(layer.mlp.down_proj, expansion_factor)\n",
    "        \n",
    "    print(\">>> 替换完成。\")\n",
    "    return model\n",
    "def merge_mpo_back_to_qwen(model):\n",
    "    \"\"\"\n",
    "    遍历模型，将所有的 OverParameterizedLinear 层合并回标准的 nn.Linear。\n",
    "    这是为了：\n",
    "    1. 恢复模型原始结构，使其能被 AutoModel 加载。\n",
    "    2. 移除超参数化带来的额外参数，恢复推理速度。\n",
    "    \"\"\"\n",
    "    print(\">>> 开始将 MPO 层合并回标准 Linear 层...\")\n",
    "    \n",
    "    # 遍历 Qwen 的所有层\n",
    "    for i, layer in enumerate(model.model.layers):\n",
    "        # 检查 MLP 的三个投影层\n",
    "        for proj_name in [\"gate_proj\", \"up_proj\", \"down_proj\"]:\n",
    "            # 获取当前模块 (可能是 nn.Linear 也可能是 OverParameterizedLinear)\n",
    "            current_module = getattr(layer.mlp, proj_name)\n",
    "            \n",
    "            # 如果是我们要找的超参数化层\n",
    "            if isinstance(current_module, OverParameterizedLinear):\n",
    "                print(f\"  - Merging Layer {i} MLP {proj_name}...\")\n",
    "                \n",
    "                # 调用类内部的合并方法，得到标准的 nn.Linear\n",
    "                merged_linear = current_module.merge_to_linear()\n",
    "                \n",
    "                # 将原来的模块替换为合并后的模块\n",
    "                setattr(layer.mlp, proj_name, merged_linear)\n",
    "                \n",
    "    print(\">>> 合并完成。模型已恢复原始架构。\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7329bfae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> 正在将 Layer 22 到 Layer 23 的 MLP 转换为超参数化层...\n",
      ">>> 扩展倍率: 1.5x\n",
      "  > Initializing MPO layer: 896 -> 1344 -> 4864\n",
      "  > Reconstruction Error: 0.000002\n",
      "  > Initializing MPO layer: 896 -> 1344 -> 4864\n",
      "  > Reconstruction Error: 0.000002\n",
      "  > Initializing MPO layer: 4864 -> 1344 -> 896\n",
      "  > Reconstruction Error: 0.000002\n",
      "  > Initializing MPO layer: 896 -> 1344 -> 4864\n",
      "  > Reconstruction Error: 0.000002\n",
      "  > Initializing MPO layer: 896 -> 1344 -> 4864\n",
      "  > Reconstruction Error: 0.000002\n",
      "  > Initializing MPO layer: 4864 -> 1344 -> 896\n",
      "  > Reconstruction Error: 0.000002\n",
      ">>> 替换完成。\n",
      ">>> Running Sanity Check (Initial PPL)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:03<00:00, 30.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Initial PPL: 10.87755845785141\n",
      "Unlocked MPO layer: model.layers.22.mlp.gate_proj\n",
      "Unlocked MPO layer: model.layers.22.mlp.up_proj\n",
      "Unlocked MPO layer: model.layers.22.mlp.down_proj\n",
      "Unlocked MPO layer: model.layers.23.mlp.gate_proj\n",
      "Unlocked MPO layer: model.layers.23.mlp.up_proj\n",
      "Unlocked MPO layer: model.layers.23.mlp.down_proj\n",
      "Trainable Params: 46.45M / 514.33M\n",
      "Ratio: 9.03%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:03<00:00, 29.23it/s]t, loss=19.3, ce=1.76, kd=34.7, feat=0.248, lr=0]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final PPL before merging: 8.042026400566101\n",
      ">>> 开始将 MPO 层合并回标准 Linear 层...\n",
      "  - Merging Layer 22 MLP gate_proj...\n",
      "  - Merging Layer 22 MLP up_proj...\n",
      "  - Merging Layer 22 MLP down_proj...\n",
      "  - Merging Layer 23 MLP gate_proj...\n",
      "  - Merging Layer 23 MLP up_proj...\n",
      "  - Merging Layer 23 MLP down_proj...\n",
      ">>> 合并完成。模型已恢复原始架构。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:03<00:00, 29.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final PPL after merging: 8.042026131153106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "from transformers import DataCollatorForLanguageModeling, get_scheduler\n",
    "from dataclasses import dataclass\n",
    "from tqdm.auto import tqdm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DistillConfig:\n",
    "    temperature: float = 2.0\n",
    "    alpha_kd: float = 0.5       # logits KD 权重 λ_kd 0.5\n",
    "    alpha_feat: float = 0.1     # hidden feature 对齐权重 λ_feat\n",
    "    alpha_param: float = 0.0    # 参数重构正则权重 λ_param（先关掉）\n",
    "    use_feat_loss: bool = True  # 是否使用特征蒸馏（只对齐最后一层）\n",
    "    use_param_loss: bool = False  # 是否使用参数正则（权重对齐）\n",
    "\n",
    "distill_cfg = DistillConfig(\n",
    "    temperature=2.0,\n",
    "    alpha_kd=0.5,\n",
    "    alpha_feat=1,\n",
    "    alpha_param=0.0,          # 默认不打开参数正则，有需要再开\n",
    "    use_feat_loss=True,\n",
    "    use_param_loss=False,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "teacher = model_1_5b               # 假设已经是 int8 + device_map={0,1}\n",
    "student = apply_mpo_to_qwen(model_0_5b, num_layers_to_replace=2, expansion_factor=1.5)# MPO 学生模型（Qwen 0.6B MPO 版）\n",
    "# teacher = teacher.to(teacher_device)\n",
    "# student = student.to(student_device)\n",
    "student_dtype = student.dtype\n",
    "# ... apply_mpo_to_qwen ...\n",
    "\n",
    "# 冒烟测试：确保初始状态没崩\n",
    "print(\">>> Running Sanity Check (Initial PPL)...\")\n",
    "dataset_check = load_dataset(\"Nan-Do/code-search-net-python\", split=\"train\").select(range(20001,21001)) # 只测50条\n",
    "student.eval()\n",
    "ppl_check = compute_ppl_batch(student, tokenizer, dataset_check, target_gpu=device)\n",
    "print(f\">>> Initial PPL: {ppl_check}\") \n",
    "# 预期：这里必须接近 6.83。如果是 8.x 或 16.x，说明初始化还是有问题，不要开始训练。\n",
    "\n",
    "# ... 确认无误后，开始 optimizer 定义和 training loop ...\n",
    "\n",
    "# teacher 不更新\n",
    "teacher.eval()\n",
    "for p in teacher.parameters():\n",
    "    p.requires_grad_(False)\n",
    "\n",
    "\n",
    "max_len = 256\n",
    "batch_size = 1\n",
    "gradient_accumulation_steps = 32\n",
    "num_epochs = 1\n",
    "lr = 1e-5\n",
    "\n",
    "raw_train = dataset\n",
    "def tokenize_fn(batch):\n",
    "    out = tokenizer(\n",
    "        batch[\"code\"],\n",
    "        truncation=True,\n",
    "        max_length=max_len,\n",
    "        padding=\"max_length\",\n",
    "        return_attention_mask=True,\n",
    "    )\n",
    "    # 非 padding token 才参与 CE\n",
    "    out[\"labels\"] = [\n",
    "        [tid if m == 1 else -100 for tid, m in zip(ids, mask)]\n",
    "        for ids, mask in zip(out[\"input_ids\"], out[\"attention_mask\"])\n",
    "    ]\n",
    "    return out\n",
    "\n",
    "train_ds = raw_train.map(\n",
    "    tokenize_fn,\n",
    "    batched=True,\n",
    "    remove_columns=raw_train.column_names,\n",
    ")\n",
    "train_ds.set_format(\n",
    "    type=\"torch\",\n",
    "    columns=[\"input_ids\", \"attention_mask\", \"labels\"],\n",
    ")\n",
    "\n",
    "collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=collator,\n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "# =====================================================\n",
    "# projection（teacher hidden -> student hidden）\n",
    "# =====================================================\n",
    "\n",
    "teacher_hidden = teacher.config.hidden_size\n",
    "student_hidden = student.config.hidden_size\n",
    "\n",
    "if teacher_hidden != student_hidden:\n",
    "    projector = nn.Linear(\n",
    "        teacher_hidden,\n",
    "        student_hidden,\n",
    "        bias=False,\n",
    "        device=device,\n",
    "        dtype=student_dtype,\n",
    "    )\n",
    "else:\n",
    "    projector = nn.Identity().to(device)\n",
    "\n",
    "# =====================================================\n",
    "# 参数正则：只针对一小部分层（示例：lm_head）\n",
    "# 先构造一个 param_pairs 列表，用于 L_param\n",
    "# =====================================================\n",
    "\n",
    "param_pairs = []\n",
    "if distill_cfg.use_param_loss and distill_cfg.alpha_param > 0.0:\n",
    "    # 示例：对齐 lm_head 权重（按你模型的真实名字来改）\n",
    "    with torch.no_grad():\n",
    "        t_lm_head_w = teacher.lm_head.weight.to(device, dtype=student_dtype)\n",
    "    param_pairs.append((student.lm_head.weight, t_lm_head_w))\n",
    "\n",
    "# =====================================================\n",
    "# 定义各个 loss 模块\n",
    "# =====================================================\n",
    "\n",
    "mse = nn.MSELoss()\n",
    "\n",
    "def compute_ce_loss(student_output):\n",
    "    \"\"\"\n",
    "    L_CE：student 对真实 token 的交叉熵。\n",
    "    HF 的 causal LM 模型在 forward 时已经返回了 loss。\n",
    "    \"\"\"\n",
    "    return student_output.loss\n",
    "\n",
    "def compute_kd_loss(student_logits, teacher_logits, T, attention_mask):\n",
    "    \"\"\"\n",
    "    修正版：只计算非 Padding 部分的 KD Loss\n",
    "    \"\"\"\n",
    "    # 1. 计算软标签概率\n",
    "    s = (student_logits / T).log_softmax(dim=-1)\n",
    "    t = (teacher_logits / T).softmax(dim=-1)\n",
    "    \n",
    "    # 2. 计算逐个 Token 的 KL 散度 [Batch, Seq_Len]\n",
    "    # KL = sum(p * (log_p - log_q)) = sum(p * log_p - p * log_q)\n",
    "    # 这里用简化的 sum(-t * s) 形式（忽略 t*log_t 常数项对梯度的影响）\n",
    "    kl_pointwise = torch.sum(-t * s, dim=-1) \n",
    "    \n",
    "    # 3. 应用 Mask [Batch, Seq_Len]\n",
    "    # attention_mask 形状通常是 [Batch, Seq_Len]，1为有效，0为Pad\n",
    "    mask = attention_mask.to(kl_pointwise.dtype)\n",
    "    \n",
    "    # 4. 只取有效 Token 的平均值\n",
    "    loss = (kl_pointwise * mask).sum() / mask.sum().clamp_min(1.0)\n",
    "    \n",
    "    return loss * (T * T)\n",
    "\n",
    "\n",
    "def compute_feat_loss(s_last_hidden, t_last_hidden_projected, attention_mask=None):\n",
    "    # 对特征进行 L2 归一化，消除模长影响，只关注方向一致性\n",
    "    s_norm = F.normalize(s_last_hidden, p=2, dim=-1)\n",
    "    t_norm = F.normalize(t_last_hidden_projected, p=2, dim=-1)\n",
    "    \n",
    "    if attention_mask is None:\n",
    "        return mse(s_norm, t_norm)\n",
    "\n",
    "    mask = attention_mask.unsqueeze(-1).to(s_last_hidden.dtype)\n",
    "    s_masked = s_norm * mask\n",
    "    t_masked = t_norm * mask\n",
    "    \n",
    "    # 归一化后的 MSE，数值通常很小 (<1.0)\n",
    "    denom = mask.sum().clamp_min(1.0)\n",
    "    # 注意：归一化后 MSE = 2 * (1 - cosine_similarity)\n",
    "    return ((s_masked - t_masked) ** 2).sum() / denom\n",
    "\n",
    "def compute_param_loss(param_pairs):\n",
    "    \"\"\"\n",
    "    L_param：参数正则（权重对齐）。\n",
    "    param_pairs: List[(W_student, W_teacher_fixed)]\n",
    "    通常只选很少的一两层做（例如 lm_head 或某几层 MPO）。\n",
    "    \"\"\"\n",
    "    if not param_pairs:\n",
    "        return torch.tensor(0.0, device=device)\n",
    "\n",
    "    loss = torch.tensor(0.0, device=device)\n",
    "    for w_s, w_t in param_pairs:\n",
    "        loss = loss + ((w_s - w_t) ** 2).mean()\n",
    "    return loss\n",
    "\n",
    "# =====================================================\n",
    "# 优化器 & Scheduler\n",
    "# =====================================================\n",
    "num_update_steps_per_epoch = len(train_loader) // gradient_accumulation_steps\n",
    "num_training_steps = num_epochs * num_update_steps_per_epoch\n",
    "\n",
    "if ONLYTRAINMPO:\n",
    "    # 1. 先冻结 Student 的所有参\n",
    "    for param in student.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # 2. 遍历模型，解冻 OverParameterizedLinear 层\n",
    "    trainable_params_count = 0\n",
    "    all_params_count = 0\n",
    "\n",
    "    for name, module in student.named_modules():\n",
    "        # 如果是我们在 MPO 替换中加入的层\n",
    "        if isinstance(module, OverParameterizedLinear):\n",
    "            # 解冻 A 和 B\n",
    "            module.matrix_a.requires_grad = True\n",
    "            module.matrix_b.requires_grad = True\n",
    "            # 如果有 bias 也解冻\n",
    "            if module.bias is not None:\n",
    "                module.bias.requires_grad = True\n",
    "                \n",
    "            print(f\"Unlocked MPO layer: {name}\")\n",
    "\n",
    "    # 3. Projector 必须训练\n",
    "    for param in projector.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    # 统计一下可训练参数量\n",
    "    for param in student.parameters():\n",
    "        all_params_count += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params_count += param.numel()\n",
    "            \n",
    "    print(f\"Trainable Params: {trainable_params_count / 1e6:.2f}M / {all_params_count / 1e6:.2f}M\")\n",
    "    print(f\"Ratio: {trainable_params_count / all_params_count * 100:.2f}%\")\n",
    "\n",
    "    # 4. 重新定义 Optimizer (只包含 requires_grad=True 的参数)\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        list(filter(lambda p: p.requires_grad, student.parameters())) + list(projector.parameters()),\n",
    "        lr=5e-5\n",
    "    )\n",
    "\n",
    "    # Scheduler 保持不变\n",
    "    lr_scheduler = get_scheduler(\n",
    "        \"linear\",\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=100,\n",
    "        num_training_steps=num_training_steps,\n",
    "    )\n",
    "else:\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        list(student.parameters()) + list(projector.parameters()),\n",
    "        lr=lr, # 建议保持 1e-5 或稍微降低到 5e-6\n",
    "    )\n",
    "\n",
    "    lr_scheduler = get_scheduler(\n",
    "        \"linear\",\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=100,            # <--- 在这里设置 Warmup\n",
    "        num_training_steps=num_training_steps,\n",
    "    )\n",
    "\n",
    "# =====================================================\n",
    "# 训练循环：双 GPU 蒸馏\n",
    "#   - teacher 用 device_map 多卡，只出 logits (+ 可选 hidden)\n",
    "#   - student 固定在 cuda:1\n",
    "# =====================================================\n",
    "\n",
    "student.train()\n",
    "projector.train()\n",
    "\n",
    "progress = tqdm(range(num_training_steps))\n",
    "# 确保在循环开始前清空梯度\n",
    "optimizer.zero_grad()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for step, batch in enumerate(train_loader):\n",
    "        # ... (数据搬运部分保持不变) ...\n",
    "        batch_cpu = {k: v for k, v in batch.items()}\n",
    "        batch_student = {k: v.to(device, non_blocking=True) for k, v in batch_cpu.items()}\n",
    "        attention_mask = batch_student[\"attention_mask\"]\n",
    "\n",
    "        # ... (Teacher Forward 保持不变) ...\n",
    "        with torch.no_grad():\n",
    "            batch_teacher = {\n",
    "                k: v.to(device, non_blocking=True)\n",
    "                for k, v in batch_cpu.items()\n",
    "                if k in (\"input_ids\", \"attention_mask\")\n",
    "            }\n",
    "            t_out = teacher(\n",
    "                **batch_teacher,\n",
    "                output_hidden_states=distill_cfg.use_feat_loss,\n",
    "                use_cache=False,\n",
    "            )\n",
    "            t_logits = t_out.logits\n",
    "            t_last_hidden = t_out.hidden_states[-1] if distill_cfg.use_feat_loss else None\n",
    "\n",
    "        # ... (Student Forward 保持不变) ...\n",
    "        s_out = student(\n",
    "            **batch_student,\n",
    "            output_hidden_states=distill_cfg.use_feat_loss,\n",
    "            use_cache=False,\n",
    "        )\n",
    "        s_logits = s_out.logits\n",
    "        s_last_hidden = s_out.hidden_states[-1] if distill_cfg.use_feat_loss else None\n",
    "\n",
    "        # ... (Loss 计算保持不变) ...\n",
    "        loss_ce = compute_ce_loss(s_out)\n",
    "        \n",
    "        # 修正版 KD (带 mask)\n",
    "        loss_kd = compute_kd_loss(\n",
    "            s_logits, \n",
    "            t_logits, \n",
    "            T=distill_cfg.temperature, \n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "\n",
    "        # 归一化版 Feat\n",
    "        if distill_cfg.use_feat_loss and t_last_hidden is not None:\n",
    "            # === 核心修改：强制转换 Teacher 的输出类型以匹配 Projector ===\n",
    "            t_last_hidden_cast = t_last_hidden.to(dtype=projector.weight.dtype) \n",
    "            \n",
    "            t_last_proj = projector(t_last_hidden_cast)\n",
    "            \n",
    "            loss_feat = compute_feat_loss(\n",
    "                s_last_hidden,\n",
    "                t_last_proj,\n",
    "                attention_mask=attention_mask,\n",
    "            )\n",
    "        else:\n",
    "            loss_feat = torch.tensor(0.0, device=device)\n",
    "\n",
    "        if distill_cfg.use_param_loss:\n",
    "            loss_param = compute_param_loss(param_pairs)\n",
    "        else:\n",
    "            loss_param = torch.tensor(0.0, device=device)\n",
    "\n",
    "        # 总 Loss\n",
    "        loss = (\n",
    "            loss_ce\n",
    "            + distill_cfg.alpha_kd * loss_kd\n",
    "            + distill_cfg.alpha_feat * loss_feat\n",
    "            + distill_cfg.alpha_param * loss_param\n",
    "        )\n",
    "\n",
    "        if torch.isnan(loss):\n",
    "            print(f\"Skipping step {step} due to NaN loss\")\n",
    "            continue\n",
    "\n",
    "        # ---------- 6. 反向 & 更新 (逻辑修正) ----------\n",
    "        \n",
    "        # 1. Loss 归一化 (分摊到每一步)\n",
    "        loss = loss / gradient_accumulation_steps\n",
    "        \n",
    "        # 2. 反向传播 (每一步都做，梯度会累积在 .grad 中)\n",
    "        loss.backward()\n",
    "\n",
    "        # 3. 参数更新 (只在累积够了步数后做)\n",
    "        if (step + 1) % gradient_accumulation_steps == 0:\n",
    "            # 梯度裁剪\n",
    "            torch.nn.utils.clip_grad_norm_(student.parameters(), 1.0)\n",
    "            \n",
    "            # 更新参数\n",
    "            optimizer.step()\n",
    "            \n",
    "            # 更新学习率\n",
    "            lr_scheduler.step()\n",
    "            \n",
    "            # 清空梯度 (为下一轮累积做准备)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # 更新进度条\n",
    "            progress.update(1)\n",
    "            progress.set_postfix({\n",
    "                \"loss\": loss.item() * gradient_accumulation_steps, # 还原数值方便观察\n",
    "                \"ce\": loss_ce.item(),\n",
    "                \"kd\": loss_kd.item(),\n",
    "                \"feat\": loss_feat.item(),\n",
    "                \"lr\": optimizer.param_groups[0]['lr']\n",
    "            })\n",
    "\n",
    "\n",
    "del teacher\n",
    "import gc\n",
    "gc.collect() \n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "student.eval()\n",
    "\n",
    "ppl = compute_ppl_batch(student, tokenizer, dataset2, target_gpu=device)\n",
    "print(f\"Final PPL before merging: {ppl}\")\n",
    "student = merge_mpo_back_to_qwen(student)\n",
    "ppl = compute_ppl_batch(student, tokenizer, dataset2, target_gpu=device)\n",
    "print(f\"Final PPL after merging: {ppl}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "69c40f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "if SAVEMODEL:\n",
    "    save_dir = \"distilled_qwen2_0_5b_mpo_kd_modular\"\n",
    "    student.save_pretrained(save_dir)\n",
    "    tokenizer.save_pretrained(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "76d4532b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "625\n"
     ]
    }
   ],
   "source": [
    "print(num_update_steps_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "592577e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOADRUNPPL:\n",
    "        \n",
    "    import os, glob\n",
    "    from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "    import torch\n",
    "    from datasets import load_dataset\n",
    "\n",
    "    save_dir = \"distilled_qwen2_0_5b_kd_modular\"  # 替换为你的保存目录\n",
    "\n",
    "    # 1) Inspect the save directory for safetensors/pytorch files\n",
    "    files = glob.glob(os.path.join(save_dir, \"*\"))\n",
    "    print(\"Saved files:\", files)\n",
    "    safetensors_files = [f for f in files if f.endswith(\".safetensors\")]\n",
    "    if safetensors_files:\n",
    "        print(\"Found safetensors files:\", safetensors_files)\n",
    "    else:\n",
    "        print(\"No .safetensors files found. Looking for pytorch weights...\")\n",
    "        pt_files = [f for f in files if \"pytorch_model\" in f or f.endswith(\".bin\")]\n",
    "        print(\"Pytorch files:\", pt_files)\n",
    "\n",
    "    # 2) Load the tokenizer\n",
    "    tokenizer_distilled = AutoTokenizer.from_pretrained(save_dir, trust_remote_code=True)\n",
    "    if tokenizer_distilled.pad_token is None:\n",
    "        tokenizer_distilled.pad_token = tokenizer_distilled.eos_token\n",
    "\n",
    "    # 3) Load the model (AutoModelForCausalLM handles safetensors automatically if present)\n",
    "    # Use device_map to put weights on the same device as `device` variable defined in the notebook.\n",
    "    # If you want to load to CPU first, set device_map=\"cpu\"\n",
    "    try:\n",
    "        model_distilled = AutoModelForCausalLM.from_pretrained(\n",
    "            save_dir,\n",
    "            trust_remote_code=True,\n",
    "            device_map=device,   # uses device variable from the notebook (e.g., \"cuda:0\")\n",
    "            low_cpu_mem_usage=True\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(\"Loading with device_map=device failed:\", e)\n",
    "        print(\"Retrying with device_map='auto'...\")\n",
    "        model_distilled = AutoModelForCausalLM.from_pretrained(\n",
    "            save_dir,\n",
    "            trust_remote_code=True,\n",
    "            device_map=\"auto\",\n",
    "            low_cpu_mem_usage=True\n",
    "        )\n",
    "    dataset = load_dataset(\"Nan-Do/code-search-net-python\", split=\"train\").select(range(10000,11000)) # 1000 样本用于测试 PPL\n",
    "    print(compute_ppl_batch(model_distilled, tokenizer_distilled, dataset))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a0b0f007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss = alpha_kd * loss_kd + alpha_feat * feat_loss + (1 - alpha_kd - alpha_feat) * loss_lm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d5ae0f49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write a short Python function to compute Fibonacci numbers. def fibonacci(n):\n",
      "    if n < 0:\n",
      "        print(\"Incorrect input\")\n",
      "    elif n == 1:\n",
      "        return 0\n",
      "    elif n == 2:\n",
      "        return 1\n",
      "    else:\n",
      "        return fibonacci(n-1) + fibonacci(n-2)\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Write a short Python function to compute Fibonacci numbers.\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(student.device)\n",
    "with torch.no_grad():\n",
    "    outputs = student.generate(**inputs, max_new_tokens=200)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qwen310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
